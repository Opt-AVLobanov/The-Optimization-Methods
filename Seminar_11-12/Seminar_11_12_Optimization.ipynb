{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Семинар 11-12\n",
        "# Метод сопряженных направлений. Метод сопряженных градиентов"
      ],
      "metadata": {
        "id": "4GyccOHKPxc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вопрос**: Метод сопряженных направлений лучше, чем метод градиентного спуска (GD)? Метод сопряженных градиентов (CG) лучше, чем метод градиентного спуска (GD)?"
      ],
      "metadata": {
        "id": "4gnkGVR_QdnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Постановка задачи"
      ],
      "metadata": {
        "id": "MWwfbFL3RvWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим квадратичную задачу:\n",
        "\n",
        "$$f(x) = \\frac{1}{2} x^T Ax - b^T x + c \\;\\;\\; → \\min_{x \\in \\mathbb{R}^n},$$\n",
        "\n",
        "где $A \\in \\mathbb{S}_{++}$."
      ],
      "metadata": {
        "id": "tjkKpKL2R0fJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Метод сопряжённых направлений "
      ],
      "metadata": {
        "id": "4DZ11zeuoYj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В градиентном спуске направления убывания - анти-градиенты, но для функций с плохо обусловленным гессианом сходимость медленная.\n",
        "\n",
        "**Главная идея метода**: это двигаться вдоль направлений, которые гарантируют сходимость за $n$ шагов."
      ],
      "metadata": {
        "id": "ikUa_GC7ocZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Def.** Множество ненулевых векторов $\\{ d_0, ..., d_k \\}$ называется сопряжённым относительно матрицы $A \\in \\mathbb{S}_{++}$, если\n",
        "$$ d_{i}^T A d_{j} = 0, \\;\\;\\; i \\neq j $$.\n"
      ],
      "metadata": {
        "id": "FJvORg5Xo7hQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Псевдокод алгоритма** сопряженных направлений:"
      ],
      "metadata": {
        "id": "zVpqAWGGq3vi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr-6_BCjPr-k"
      },
      "outputs": [],
      "source": [
        "def ConjugateDirections(x0, A, b, p):\n",
        "    x = x0\n",
        "    r = A.dot(x) - b\n",
        "    for i in xrange(len(p)):\n",
        "        alpha = - (r.dot(p[i])) / (p[i].dot(A.dot(p[i])))\n",
        "        x = x + alpha * p[i]\n",
        "        r = A.dot(x) - b\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вопрос**: Какой недостаток у данного метода?\n",
        "Возможно его улучшить?"
      ],
      "metadata": {
        "id": "EEQMRBvdsP9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Метод сопряженных градиентов (Conjugate gradient method)"
      ],
      "metadata": {
        "id": "sA01t2_9scVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Главная идея метода**: новое направление $d_k$ ищется в виде $d_k = u_k + \\sum_{j=0}^{k-1}\\beta_{k,j}d_{j}$. Тогда, взяв за ветора $u_k = r_k$, получится избавится от недостатка метода сопряженных направлений и новое направление будет считаться следующим образом:\n",
        "\\begin{equation*}\n",
        "  d_k = r_k + \\beta_{k}d_{k-1}, \\;\\;\\; \\text{где } \\beta_k = \\frac{r_k^T r_k}{r_{k-1}^T r_{k-1}} \n",
        "\\end{equation*}"
      ],
      "metadata": {
        "id": "mqkJPdkbJIsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Таким образом, для получения следующего сопряжённого направления $d_k$ необходимо хранить только сопряжённое направление $d_{k-1}$ и остаток $r_{k}$ с предыдущей итерации."
      ],
      "metadata": {
        "id": "dqe_dqIaLWvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Псевдокод алгоритма** сопряженных градиентов:"
      ],
      "metadata": {
        "id": "WByQZFMhLk1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ConjugateGradientQuadratic(x0, A, b):\n",
        "    r = b - A.dot(x0)\n",
        "    d = r\n",
        "    while np.linalg.norm(r) != 0:\n",
        "        alpha = r.dot(r) / d.dot(A.dot(d))\n",
        "        x = x + alpha * d\n",
        "        r_next = r - alpha * A.dot(d)\n",
        "        beta = r_next.dot(r_next) / r.dot(r)\n",
        "        d = r_next + beta * d\n",
        "        r = r_next\n",
        "    return x"
      ],
      "metadata": {
        "id": "pv0_k5HpsbOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Теоремы сходимости"
      ],
      "metadata": {
        "id": "7PDbSQQtMGsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Теорема 1.** Если матрица $A$ имеет только $m$ различных собственных значений, то метод сопряжённых градиентов cойдётся за $m$ итераций.\n",
        "\n",
        "**Теорема 2.** Имеет место следующая оценка сходимости:\n",
        "\\begin{equation*}\n",
        "  \\| x_{k+1} - x^* \\|_A \\leq \\left( \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k \\| x_0 - x^* \\|_A,\n",
        "\\end{equation*}\n",
        "где $ \\|x \\|_A = x^TAx $ и $\\kappa(A) = \\|A \\| \\cdot \\|A^{-1} \\| = \\frac{\\lambda_{max}}{\\lambda_{min}}$."
      ],
      "metadata": {
        "id": "wRnVNHitMJb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод сопряженных градиентов для неквадратичной функции:"
      ],
      "metadata": {
        "id": "RWdFQeccQirm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Главная идея метода**: использовать анти-градиенты $-f'(x_k)$ неквадратичной функции вместо остатков $r_k$ и линейный поиск шага $α_k$ вместо аналитического вычисления. Получим метод Флетчера-Ривса."
      ],
      "metadata": {
        "id": "-MjAUo4TRKLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ConjugateGradientQuadratic(x0, A, b):\n",
        "    r = -gradf(x)\n",
        "    d = r\n",
        "    while np.linalg.norm(r) != 0:\n",
        "        alpha = StepSearch(x, f, gradf, **kwargs)\n",
        "        x = x + alpha * d\n",
        "        r_next = -gradf(x)\n",
        "        beta = r_next.dot(r_next) / r.dot(r)\n",
        "        d = r_next + beta * d\n",
        "        r = r_next\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "vQbMhshCMC6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Использованные материалы:\n",
        "\n",
        "*   [An Introduction to the Conjugate Gradient Method](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)\n",
        "*   [The Concept of Conjugate Gradient Descent in Python ](https://ikuz.eu/machine-learning-and-computer-science/the-concept-of-conjugate-gradient-descent-in-python/)\n",
        "\n"
      ],
      "metadata": {
        "id": "4lFxuAepSK7O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H2H6WJb3SF6a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}